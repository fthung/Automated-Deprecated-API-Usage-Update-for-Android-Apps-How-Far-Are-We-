In this section, we discuss some threats to validity, cases that are not
handled by our mitigations, and future directions for improving the
approach to updating deprecated API usage in Android apps.

\subsection{Threats to Validity}

One threat is related to whether we have configured AppEvolve correctly. To mitigate this threat, we have tried our best to run AppEvolve following the instructions given by the authors in the AppEvolve documentation. We have also asked AppEvolve's first author on how to configure AppEvolve correctly for new mobile apps. We can replicate all of the results on the original dataset, even when we try to redo the configurations from scratch.  We have also rechecked our configurations several times. Also, we have released a replication package\footnote{\url{...}} for others to check and validate.

Another threat is related to generalizability of the findings. We have added 54 new applications to evaluate the effectiveness of
AppEvolve in generating applicable updates. This translates to 54 API usage
locations on top of the 41 API usage locations in AppEvolve dataset, thus
more than doubling the number of API usage locations and more than triple the number of apps as compared to the dataset used in the original work. We
believe this is sufficient to understand the capabilities of AppEvolve, as
44 of the 54 API usages that we have added uncover limitations of
AppEvolve. There might be other cases that AppEvolve cannot handle, but we
believe that we have found many of them.


\subsection{Cases Unhandled by Our Mitigation Strategy}
There are 8 cases in which our mitigation strategy was not successful. For two cases where our mitigations do not work, we manage to manually alter the target code such that
AppEvolve can successfully apply the learned edits. We show these two cases below.

For one case, we modified the code on the left hand side of Listing 7 in Table~\ref{tab:mitigatefail} to the one below. In the modified code, a static field {\tt afChangeListener} of type {\tt AudioManager.OnAudioFocusChangeListener} is directly put as the first argument of {\tt requestAudioFocus} and AppEvolve cannot generate an applicable update. When {\tt afChangeListener} is assigned to a local variable named {\tt af}, AppEvolve can generate an applicable update.
\begin{lstlisting}[language=text,numbers=none]
private static AudioManager mAudioManager;
private static OnAudioFocusChangeListener afChangeListener;
public static void requestAudioFocus
    (Context context) {
    if(mAudioManager == null){
        mAudioManager = (AudioManager) context
            .getApplicationContext()
            .getSystemService(Context.AUDIO_SERVICE);
    }
-   mAudioManager.requestAudioFocus(
-       afChangeListener,
-       AudioManager.STREAM_MUSIC,
-       AudioManager.AUDIOFOCUS_GAI
+   int res;
+   int a = AudioManager.STREAM_MUSIC;
+   int b = AudioManager.AUDIOFOCUS_GAIN;
+   AudioManager am = mAudioManager;
+   AudioManager.OnAudioFocusChangeListener
+        af = afChangeListener;
+    res = am.requestAudioFocus(af,a,b);
}

\end{lstlisting}


We modify the code in the left hand side of Listing 8 in Table~\ref{tab:mitigatefail} to the one below.
\begin{lstlisting}[language=text,numbers=none]
private Paint paint;
  @Override protected void onDraw(Canvas canvas) {
    super.onDraw(canvas);
    canvas.drawColor(Color.GREEN);
+   float a = 0;
+   float b = 0;
+   float c = getWidth();
+   float d = getHeight();
+   int flag = Canvas.CLIP_SAVE_FLAG;
+   canvas.saveLayer(a,b,c,d, paint, flag);
-   canvas.saveLayer(0, 0, canvas.getWidth(),
-       canvas.getHeight(), paint,
-       Canvas.ALL_SAVE_FLAG);
    canvas.restore();
  }
\end{lstlisting}
In the above listing, a field named {\tt paint} of type {\tt Paint} has a {\tt final} modifier. Removing this modifier results in an applicable update. It suggests that AppEvolve does not support {\tt final} modifier.

\subsection{Result assessment}

We were surprised at the large difference between the results on our data
set, where we obtained an accuracy \jl{right word?} of \jl{what}\%, and the
results reported by the authors of AppEvolve, who obtained an accuracy of
\jl{what}\%, even though our dataset was constructed randomly, without
taking into account the strengths or weaknesses of AppEvolve.  Likewise,
given that AppEvolve relies on matching statements, abstracting only over
variables, between the examples and the target, it is surprising that
AppEvolve could achieve such high accuracy on simple getter functions such
as \jl{getCurrentHour()}, that in our dataset are often used as
subexpressions of arbitrary possibly application-specific statement, and on
method calls that have arbitrary, again project-specific, arguments, such
as our example in Listing \jl{which?}.

To understand better how AppEvolve overcomes these challenges, we
investigate the examples and target programs used in the original AppEvolve
evaluation.  Given the difficulty of finding the original files for the
examples, for which no origin information is provided, we focus on a single
example, the call to \jl{getCurrentHour()} in \jl{what project} in F-Droid
version \jl{which} (\jl{what code} in the AppEvolve paper).  In F-Droid, we
find that the call to \jl{getCurrentHour()} is found in the argument list
of a call to \jl{what?}, a code structure similar to our Listing
\jl{which}.  In the AppEvolve dataset, this call is extracted into a
separate statement, analogous to our mitigation shown on the right hand
side of Listing \jl{which}.  We have furthermore looked at one of the
examples provided for this API, and have found the original on GitHub i the
file \jl{fill in}.  Again, we find that in the original example, the call
appears in an argument list, which the corresponding example in the
AppEvolve example set has been transformed analogous to our mitigation.
This change converts invocations that occur in differing contexts to
invocations that can be matched by AppEvolve.

A solution, to retain the spirit of AppEvolve, could be to apply our
mitigation systematically to the occurrences of the deprecated API in the
examples and in the target code.  Such normalizations may, however,
introduce readability issues.  We leave this to future work.

\jl{The last paragraph could be adjusted to fit better with the context.
It is important to make clear that it doesn't matter what is the form of
the target program and the examples.  What is important is that they all
have the same form.}


\subsection{Future Directions}
Our mitigations can fix 81.82\% of the failed updates that we found,
showing that even simple modifications of the target apps can allow
edits that are learned from examples of API usage to be applied
successfully. This technique can possibly be generalized by normalizing the
code into a standard form. Edits can then be learned from this standard
form. When the edit is applied to a new piece of code, that target code
should also be normalized, which should minimize the variations in the code
due to simple refactorings. If the edits are successfully applied, the
resulting code can then be refactored again to restore the coding
style of the developers.  Our mitigations represent a subset of the normalizations that are
possible.

Related to the last point on refactoring resulting code to follow the coding style of developers, we perform a simple experiment to compare the output of AppEvolve after our simple refactoring with code that are manually updated to deal with a set of deprecated API.  We
asked a software engineer with ... years of Java experience and who is not an author of this paper to update the code shown on the left hand side of Listing ... in Table~\ref{tab:mitigatesucc}. The code generated by AppEvolve and by the engineer are shown below.

%Buse and Weimer~\cite{Buse:2008:MSR:1390630.1390647} propose a metric of
%software readability that are based on a set of simple local features. They
%also provide an automatic readability measure model that is built on 12,000
%code readability judgments. We run this model and compare the readability
%measure of the API update created by AppEvolve against that of the update
%created by the human.  Scores range from 0 to 1 where, 0 is least and 1 is
%most readable.

\vspace{0.2cm}\noindent {\bf AppEvolve's Update}
\begin{lstlisting}[language=text,numbers=none]
public void displayTime(View view){
  int varInt1;
  int varInt2;
  if (android.os.Build.VERSION.SDK_INT >= android.os.Build.VERSION_CODES.M) {
    varInt1=timePicker.getHour();
  }
 else {
    varInt1=timePicker.getCurrentHour();
  }
  if (android.os.Build.VERSION.SDK_INT >= android.os.Build.VERSION_CODES.M) {
    varInt2=timePicker.getMinute();
  }
 else {
    varInt2=timePicker.getCurrentMinute();
  }
  String time=varInt1 + ":" + varInt2;
  Toast.makeText(this,time,Toast.LENGTH_SHORT).show();
}
\end{lstlisting}


\vspace{0.2cm}\noindent {\bf Engineer's Update}
\begin{lstlisting}[language=text,numbers=none]
public void displayTime(View view) {
    String time;
    if (android.os.Build.VERSION.SDK_INT >= 23) {
        // only for gingerbread and newer versions
        time = timePicker.getHour() + ":" + timePicker.getMinute();
    } else {
        time = timePicker.getCurrentHour() + ":" + timePicker.getCurrentMinute();
    }
    Toast.makeText(this, time, Toast.LENGTH_SHORT).show();
}
\end{lstlisting}

\jl{the following two paragraphs have to be merged.}
The code made manually by the engineer is different from the code produced
by AppEvolve. Indeed, developers may prefer the code made the
engineer. Future work may want investigate methods that can effectively
refactor AppEvolve generated code to better fit into the coding style of
developers. One step would be to choose good names for the newly introduced
variables. A number of recent studies use approaches based on deep learning
and mining software repositories to recommend method and class
names~\cite{...}. These approaches can potentially be extended for inferring good
variable names in our context.

As you can see, the code made by the engineer is different than AppEvolve's. Developers may prefer the code made by the engineer. Future work may want to investigate methods that can effectively refactor AppEvolve generated code to better fit into the coding style of developers. One future work that can be done is to design methods to infer good names to newly introduced variables. There have been a number of studies that use deep learning and mining software repository approaches to recommend method and class names~\cite{allamanis2015suggesting}. These work can potentially be extended for inferring good variable names.


\jl{why was this paragraph removed?}
Moreover, in this work, we have assumed it is safe to move a method
invocation from a location to another earlier location. Such a rewriting
may have unintended consequences in the presence of side effects, if the
transformation changes the order in which code is executed.  For example,
suppose we move a method invocation $M_3$ that appears as a third argument
of another method invocation $M$. For this case, if the evaluation of the
first and the second arguments of $M$ influence the internal state of the
program that affects the outcome of $M_3$, the refactoring that we did may
lead to an erroneous program state. In the future, we may want to extend
the line of work on side effect analysis and purity inference, e.g., to
deal with this issue. Note that, in our dataset, we do not have such a
case.  Indeed, it is typically not considered to be good programming style
to rely on the order of evaluation of function arguments.





%The difference in readability is very small. Indeed, the human developer
%also implicitly performed the normalization, and the differences between
%the variants are due to the choice of variable names, the spacing around
%function arguments and the degree of indentation.  Recent research has
%proposed strategies to automate the selection of new variable
%names~\cite{...}.  The other issues can be addressed by tools such as
%\jl{indent?  Apparently it only targets C programs} that are
%parameterizable according to the desired coding style.
