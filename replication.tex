The goal of our replication is to find out whether the effectiveness of AppEvolve is generalizable.
\subsection{Dataset}
We include additional mobile apps that use deprecated APIs in original AppEvolve dataset and does not use APIs that are suggested to replace the deprecated APIs. In this way, we use the same training data (i.e., examples of API usages) and introduce a new test data (i.e., mobile apps) to evaluate whether AppEvolve can learn edits and apply them well in the new usage locations in the test data. We find the new mobile apps by querying GitHub Code Search\footnote{\url{https://github.com/search}} using the name of the APIs. GitHub Code Search returns a list of ranked files matching the query. Since Github Code Search only supports textual queries, we have many false positive results (i.e., the files do not contain the API that are queried for) since the query may match any text inside a source code file. Thus, we manually check the returned files to confirm that they actually contain the usages of APIs that we want. Finally, we randomly selected 54 API usages from 54 apps that we found as our dataset, as shown in Table~\ref{}.

One may be concerned that the mobile apps that we found may contain examples of API usage that were used by AppEvolve since it searches GitHub to find such examples. For such cases, applying the edits are easy since the code is exactly the same as the example. However, such cases do not occur since GitHub Code Search indexes the latest version of the code. Thus, we will not find examples of API usage since we target only mobile apps that use deprecated APIs but not their replacement APIs.

\subsection{Procedure}
We need to create an AppEvolve configuration for each app in our dataset. To do so, we carefully read and understand the existing AppEvolve configurations that were used in AppEvolve paper's experiment. We asked the first author of AppEvolve to confirm how to configure the apps correctly. We also ran our experiments in the same virtual machine environment provided by the authors.\footnote{\url{https://sites.google.com/view/appevolve}}

After manually configuring the mobile apps, we ran AppEvolve on them. We recorded the number of applicable and failed updates. For the failed updates, we categorized them using card sorting\cite{...}. In card sorting, we perform multiple passes on the failed update data. For the first pass, we put each of the failed updates into a category that we created on the spot by analyzing what a failed update or a collection of them are all about. For subsequent passes, we reevaluated the categories. We might rename a category to be more descriptive of the problem that occurs in the set of updates belonging to the category. We might also merge related categories into one. These steps were repeated until there were no more changes to the category.